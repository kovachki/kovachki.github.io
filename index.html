<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Nikola B. Kovachki


</title>
<meta name="description" content="Personal webpage of Nikola B. Kovachki.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üå≥Ô∏è</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
        <!-- Social Icons -->
        <div class="navbar-brand social">
          <a href="mailto:%6E%6B%6F%76%61%63%68%6B%69@%6E%76%69%64%69%61.%63%6F%6D"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=redLUcgAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>



<a href="https://www.linkedin.com/in/nikola-kovachki-85a0b2225" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>














        </div>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/CV/">
                CV
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/talks/">
                talks
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Nikola</span> B. Kovachki
    </h1>
     <p class="desc"><a href="https://www.nvidia.com/en-us/research/" target="_blank" rel="noopener noreferrer">NVIDIA</a></p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
        <div class="address">
          <p>Los Angeles, CA</p> <p style="font-size:80%;">nkovachki (at) nvidia (dot) com</p>

        </div>
      
    </div>
    

    <div class="clearfix">
      <p>I am research scientist at NVIDIA Research (NVR) working on machine learning methods for the
physical sciences in theory and practice. I obtained my Ph.D. in applied and computational mathematics 
from Caltech in 2022 under the supervision of Prof. <a href="http://stuart.caltech.edu/" target="_blank" rel="noopener noreferrer">Andrew M. Stuart</a>.
Previously, I received a B.Sc. in mathematics from Caltech in 2016. I am a recipient of the 
2020 <a href="https://www.cms.caltech.edu/academics/honors#amazon" target="_blank" rel="noopener noreferrer">Amazon AI4Science Fellowship</a>,
and some of my work has been featured in popular science magazines:
 <a href="https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/" target="_blank" rel="noopener noreferrer">MIT Technology Review</a>, 
 <a href="https://www.quantamagazine.org/latest-neural-nets-solve-worlds-hardest-equations-faster-than-ever-before-20210419/" target="_blank" rel="noopener noreferrer">Quanta Magazine</a>.</p>

<p>My research interest lie at the intersection of approximation theory, numerical analysis, and machine learning.
Particularly, I work on the design and analysis of efficient approximation methods for forwards and inverse problems
in PDEs, measure transport methods for sampling in high dimensions, and the blending of data and physics into machine 
learning models.</p>

<!--
I work in collaboration with my advisor Prof. <a href="http://stuart.caltech.edu/">Andrew M. Stuart</a>
and have also collaborated with various experts in the fields of machine learning and physical modeling including Prof. <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima Anandkumar</a>,
Prof. <a href="https://mechmat.caltech.edu/">Kaushik Bhattacharya</a>, Prof. <a href="https://maartendehoop.rice.edu/">Maarten V. de Hoop</a>, 
Prof. <a href="https://uqgroup.mit.edu/">Youssef Marzouk</a>, Prof. <a href="https://millergroup.caltech.edu/Miller_Group/Home.html">Tom Miller</a>, and Prof. <a href="https://math.ethz.ch/sam/the-institute/people/siddhartha-mishra.html">Siddhartha Mishra</a>. I am originally from 
 Sofia, Bulgaria but have lived in the US since 2005 (Atlanta &rarr; LA). I received a B.Sc. in mathematics from Caltech in 2016. I am a recipient of the 
 2020 <a href="https://www.cms.caltech.edu/academics/honors#amazon">Amazon AI4Science Fellowship</a> which recognizes the outstanding work of graduate students
 in machine learning that impacts other scientific fields. My work has been written about in popular science magazines:
 <a href="https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/">MIT Technology Review</a>, 
 <a href="https://www.quantamagazine.org/latest-neural-nets-solve-worlds-hardest-equations-faster-than-ever-before-20210419/">Quanta Magazine</a>, 
 and was recently highlighted in NVIDIA CEO Jensen Huang's GTC 2021  <a href="https://youtu.be/jhDiaUL_RaM?t=2472">keynote address</a>. 

My broader interest include anything mathematically beautiful or machine learning related.
In particular, I am excited about the <a href="https://arxiv.org/pdf/2012.14501.pdf">approximation theory</a> of neural networks, the
application of data-driven techniques to <a href="https://discovery.ucl.ac.uk/id/eprint/10083845/7/Arridge_Solving%20inverse%20problems%20using%20data-driven%20models_VoR.pdf">inverse problems</a>, 
the theory and application of <a href="https://arxiv.org/pdf/2108.08481.pdf">operator learning</a> techniques for imaging and the computational
sciences, and the development of <a href="https://arxiv.org/pdf/1602.05023.pdf">uncertainty quantification</a> techniques with deep neural
networks. I am also interested in the large-scale deployment and integration of learning systems in 
super computers for more efficient physical simulations or in computationally limited hardware 
for consumer and commercial products.

I love spending my free time outdoors whenever possible. I particularly 
enjoy hiking, mountain biking, snowboarding, surfing, and skateboarding.
I've also recently become an avid runner. Indoors, I love experimenting 
with new cooking techniques (recently fermentation), experiencing all forms of 
art and music and badly attempting to create my own, and exploring LA for 
new food and coffee. Last but certainly not least, I love spending time 
with my <a href="/assets/img/nik_shushi.jpg" target="_blank">amazing dog.</a>

 <i>I am currently on the job market!</i> Please find my <a href="/assets/pdf/kovachki_cv.pdf" target="_blank">CV</a> and contact me if you think I'd be a good fit for your team.
-->

    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Oct 26, 2023</th>
          <td>
            
              I was a panelist at <a href="https://event.asme.org/InterPACK" target="_blank" rel="noopener noreferrer">InterPACK 2023</a> in the session on <i>AI for the Thermal Science Community</i>. Thank you <a href="https://engineering.uci.edu/users/yoonjin-won" target="_blank" rel="noopener noreferrer">Yoonjin Won</a> for the invitation!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 14, 2023</th>
          <td>
            
              I gave a talk on Diffusion Models in Infinite Dimensions at <a href="https://sites.google.com/view/siampnw23/home" target="_blank" rel="noopener noreferrer">SIAM PNW4</a> in the minisymposium on <i>Scientific Machine Learning</i>. Thank you <a href="https://amath.washington.edu/people/alexander-hsu" target="_blank" rel="noopener noreferrer">Alex Hsu</a> for the invitation!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Aug 28, 2023</th>
          <td>
            
              I gave a talk on Diffusion Models in Infinite Dimensions at <a href="https://iciam2023.org/" target="_blank" rel="noopener noreferrer">ICIAM 2023</a> in the minisymposium on <i>Theoretical foundations and algorithmic innovation in operator learning</i>.
Thank you <a href="https://jakobzech.com/" target="_blank" rel="noopener noreferrer">Jakob Zech</a> for the invitation!

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography">
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">JMLR</abbr>
    
  
  </div>

  <div id="kovachki2023neural" class="col-sm-8">
    
      <div class="title">Neural Operator: Learning Maps Between Function Spaces With Applications to PDEs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Kovachki, Nikola B,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Zongyi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Burigede,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Azizzadenesheli, Kamyar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bhattacharya, Kaushik,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Stuart, Andrew M,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Anandkumar, Anima
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of Machine Learning</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://www.jmlr.org/papers/v24/21-1524.html" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="https://www.jmlr.org/papers/volume24/21-1524/21-1524.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The classical development of neural networks has primarily focused on learning mappings between finite dimensional Euclidean spaces or finite sets. We propose a generalization of neural networks to learn operators, termed neural operators, that map between infinite dimensional function spaces. We formulate the neural operator as a composition of linear integral operators and nonlinear activation functions. We prove a universal approximation theorem for our proposed neural operator, showing that it can approximate any given nonlinear continuous operator. The proposed neural operators are also discretization-invariant, i.e., they share the same model parameters among different discretization of the underlying function spaces. Furthermore, we introduce four classes of efficient parameterization, viz., graph neural operators, multi-pole graph neural operators, low-rank neural operators, and Fourier neural operators. An important application for neural operators is learning surrogate maps for the solution operators of partial differential equations (PDEs). We consider standard PDEs such as the Burgers, Darcy subsurface flow, and the Navier-Stokes equations, and show that the proposed neural operators have superior performance compared to existing machine learning based methodologies, while being several orders of magnitude faster than conventional PDE solvers.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">kovachki2023neural</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{JMLR}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Neural Operator: Learning Maps Between Function Spaces With Applications to PDEs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kovachki, Nikola B and Li, Zongyi and Liu, Burigede and Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and Stuart, Andrew M and Anandkumar, Anima}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Machine Learning}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{24}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{89}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://www.jmlr.org/papers/v24/21-1524.html}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://www.jmlr.org/papers/volume24/21-1524/21-1524.pdf}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">arXiv</abbr>
    
  
  </div>

  <div id="baptista2023anapproximation" class="col-sm-8">
    
      <div class="title">An Approximation Theory Framework for Measure-Transport Sampling Algorithms</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Baptista, Ricardo,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hosseini, Bamdad,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kovachki, Nikola B,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Marzouk, Youssef M,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Sagiv, Amir
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>CoRR</em>
      
      
        2023
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://arxiv.org/abs/2302.13965" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="https://arxiv.org/pdf/2302.13965.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>This article presents a general approximation-theoretic framework to analyze measure transport algorithms for probabilistic modeling. A primary motivating application for such algorithms is sampling ‚Äì a central task in statistical inference and generative modeling. We provide a priori error estimates in the continuum limit, i.e., when the measures (or their densities) are given, but when the transport map is discretized or approximated using a finite-dimensional function space. Our analysis relies on the regularity theory of transport maps and on classical approximation theory for high-dimensional functions. A third element of our analysis, which is of independent interest, is the development of new stability estimates that relate the distance between two maps to the distance¬†(or divergence) between the pushforward measures they define. We present a series of applications of our framework, where quantitative convergence rates are obtained for practical problems using Wasserstein metrics, maximum mean discrepancy, and Kullback‚ÄìLeibler divergence. Specialized rates for approximations of the popular triangular Knothe-Rosenblatt maps are obtained, followed by numerical experiments that demonstrate and extend our theory.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">baptista2023anapproximation</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Baptista, Ricardo and Hosseini, Bamdad and Kovachki, Nikola B and Marzouk, Youssef M and Sagiv, Amir}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An Approximation Theory Framework for Measure-Transport Sampling Algorithms}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{CoRR}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{abs/2302.13965}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2302.13965}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2302.13965.pdf}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">JMPS</abbr>
    
  
  </div>

  <div id="liu2022alearningbased" class="col-sm-8">
    
      <div class="title">A Learning-based Multiscale Method and its Application to Inelastic Impact Problems</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Liu, Burigede,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kovachki, Nikola B,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Li, Zongyi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Azizzadenesheli, Kamyar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Anandkumar, Anima,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Stuart, Andrew M,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Bhattacharya, Kaushik
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>Journal of the Mechanics and Physics of Solids</em>
      
      
        2022
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://www.sciencedirect.com/science/article/pii/S0022509621002982" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="https://arxiv.org/pdf/2102.07256.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The macroscopic properties of materials that we observe and exploit in engineering application result from complex interactions between physics at multiple length and time scales: electronic, atomistic, defects, domains etc. Multiscale modeling seeks to understand these interactions by exploiting the inherent hierarchy where the behavior at a coarser scale regulates and averages the behavior at a finer scale. This requires the repeated solution of computationally expensive finer-scale models, and often a priori knowledge of those aspects of the finer-scale behavior that affect the coarser scale (order parameters, state variables, descriptors, etc.). We address this challenge in a two-scale setting where we learn the fine-scale behavior from off-line calculations and then use the learnt behavior directly in coarse scale calculations. The approach builds on the recent success of deep neural networks by combining their approximation power in high dimensions with ideas from model reduction. It results in a neural network approximation that has high fidelity, is computationally inexpensive, is independent of the need for a priori knowledge, and can be used directly in the coarse scale calculations. We demonstrate the approach on problems involving the impact of magnesium, a promising light-weight structural and protective material.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">liu2022alearningbased</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{JMPS}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Learning-based Multiscale Method and its Application to Inelastic Impact Problems}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of the Mechanics and Physics of Solids}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{158}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0022509621002982}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://arxiv.org/pdf/2102.07256.pdf}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Burigede and Kovachki, Nikola B and Li, Zongyi and Azizzadenesheli, Kamyar and Anandkumar, Anima and Stuart, Andrew M and Bhattacharya, Kaushik}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">ICLR</abbr>
    
  
  </div>

  <div id="li2021fourier" class="col-sm-8">
    
      <div class="title">Fourier Neural Operator for Parametric Partial Differential Equations</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Li, Zongyi,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kovachki, Nikola B,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Azizzadenesheli, Kamyar,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Liu, Burigede,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Bhattacharya, Kaushik,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Stuart, Andrew M,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Anandkumar, Anima
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In International Conference on Learning Representations (ICLR)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://iclr.cc/virtual/2021/poster/3281" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="https://openreview.net/pdf?id=c8P9NQVtmnO" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers‚Äô equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2021fourier</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{ICLR}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Zongyi and Kovachki, Nikola B and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew M and Anandkumar, Anima}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fourier Neural Operator for Parametric Partial Differential Equations}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Learning Representations (ICLR)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{9}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{OpenReview.net}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://openreview.net/pdf?id=c8P9NQVtmnO}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://iclr.cc/virtual/2021/poster/3281}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
<li>
<div class="row">
  <div class="col-sm-2 abbr">
  
    
    <abbr class="badge">SMAI-JCM</abbr>
    
  
  </div>

  <div id="bhattacharya2021model" class="col-sm-8">
    
      <div class="title">Model Reduction and Neural Networks for Parametric PDEs</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Bhattacharya, Kaushik,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Hosseini, Bamdad,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Kovachki, Nikola B,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Stuart, Andrew M
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>The SMAI journal of computational mathematics</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
        <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a>
    
    
      <a href="https://smai-jcm.centre-mersenne.org/articles/10.5802/smai-jcm.74/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a>
    
    
      
      <a href="https://smai-jcm.centre-mersenne.org/item/10.5802/smai-jcm.74.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
      
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>We develop a general framework for data-driven approximation of input-output maps between infinite-dimensional spaces. The proposed approach is motivated by the recent successes of neural networks and deep learning, in combination with ideas from model reduction. This combination results in a neural network approximation which, in principle, is defined on infinite-dimensional spaces and, in practice, is robust to the dimension of finite-dimensional approximations of these spaces required for computation. For a class of input-output maps, and suitably chosen probability measures on the inputs, we prove convergence of the proposed approximation methodology. We also include numerical experiments which demonstrate the effectiveness of the method, showing convergence and robustness of the approximation scheme with respect to the size of the discretization, and compare it with existing algorithms from the literature; our examples include the mapping from coefficient to solution in a divergence form elliptic partial differential equation (PDE) problem, and the solution operator for viscous Burgers‚Äô equation.</p>
    </div>
    

    <!-- Hidden bibtex block -->
    
    <div class="bibtex hidden">
      <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">bhattacharya2021model</span><span class="p">,</span>
  <span class="na">abbr</span> <span class="p">=</span> <span class="s">{SMAI-JCM}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Bhattacharya, Kaushik and Hosseini, Bamdad and Kovachki, Nikola B and Stuart, Andrew M}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Model Reduction and Neural Networks for Parametric PDEs}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{The SMAI journal of computational mathematics}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Soci\'et\'e de Math\'ematiques Appliqu\'ees et Industrielles}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{7}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">html</span> <span class="p">=</span> <span class="s">{https://smai-jcm.centre-mersenne.org/articles/10.5802/smai-jcm.74/}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://smai-jcm.centre-mersenne.org/item/10.5802/smai-jcm.74.pdf}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
    </div>
    
  </div>
</div>
</li>
</ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%6E%6B%6F%76%61%63%68%6B%69@%6E%76%69%64%69%61.%63%6F%6D"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=redLUcgAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a>



<a href="https://www.linkedin.com/in/nikola-kovachki-85a0b2225" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a>














      </div>
      <div class="contact-note"></div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    ¬© Copyright 2023 Nikola B. Kovachki.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
